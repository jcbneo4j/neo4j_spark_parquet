{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4652fb-e91c-4698-a78b-d40d08ddc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"32000M\",\"executorCores\":15,\n",
    "\"conf\": {\n",
    "    \"spark.jars\": \"s3:/<s3_bucket_name>/neo4j/neo4j-connector-apache-spark_2.11-4.1.2_for_spark_2.4.jar\"\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28a87ab7-0ae7-4d9d-8110-145ac896c372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T19:18:00.961865Z",
     "iopub.status.busy": "2022-07-12T19:18:00.961675Z",
     "iopub.status.idle": "2022-07-12T19:18:01.216704Z",
     "shell.execute_reply": "2022-07-12T19:18:01.216173Z",
     "shell.execute_reply.started": "2022-07-12T19:18:00.961841Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8f181f08dc44d29f04f418715cff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210da444-e543-4049-8c72-ca029d41d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main bucket for parquet files to read into dataframe\n",
    "final_df = spark.read.parquet(\"s3://<s3_bucket_name>/parquet_files/\")\n",
    "\n",
    "final_df.createOrReplaceTempView(\"final\")\n",
    "\n",
    "#count of dataframe\n",
    "print(f'Total Rows: {final_df.count():,}')\n",
    "\n",
    "#example of various forms of filtering\n",
    "filtered_df = final_df.filter(\"<column_name> is not null\")\n",
    "\n",
    "#count of filtered datafram\n",
    "print(f'Total Rows after filter: {filtered_df.count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "262e4748-59be-4838-ae75-71f757f71f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T19:18:08.665045Z",
     "iopub.status.busy": "2022-07-12T19:18:08.664866Z",
     "iopub.status.idle": "2022-07-12T19:18:30.011512Z",
     "shell.execute_reply": "2022-07-12T19:18:30.010977Z",
     "shell.execute_reply.started": "2022-07-12T19:18:08.665022Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684d982296a946899d9f3360c23c9fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "base configuration for neo4j\n",
    "\"\"\"\n",
    "config = {\n",
    "    \"bolt_url\": \"bolt://<neo_db_ip>>:7687\",\n",
    "    \"database\": \"<database_name>\",\n",
    "    \"user\": \"<user_name>\",\n",
    "    \"password\": \"<password>\",\n",
    "    \"batch_size\": \"500\"\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "map to descibe nodes and relationships passed into the neo read/write methods. Would like to move to yaml and maintain externally in git &/or s3 with validation.\n",
    "\"\"\"\n",
    "model_map = {\n",
    "    \"node1_node2\" : {\n",
    "        \"source\" : {\n",
    "            \"node\": \":Node1\",\n",
    "            \"property\": \"id_field:id\"\n",
    "        },\n",
    "        \"target\" : {\n",
    "            \"node\": \":Node2\",\n",
    "            \"property\": \"id_field:id\"\n",
    "        },\n",
    "        \"relationship\": {\n",
    "            \"rel\": \"IS_RELATED\",\n",
    "            \"properties\": \"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "method to accept node name as param and return a string of cypher, if needed.\n",
    "\"\"\"\n",
    "def get_cypher_query(node_name):\n",
    "    cypher_map = {\n",
    "        \"node1\": \"MERGE (n:Node1 {id: event.id_field, createdDate: event.date})\",\n",
    "        \"node2\": \"MERGE (n:Node1 {id: event.id_field, createdDate: event.date})\"\n",
    "    }\n",
    "    return cypher_map[node_name.lower()]\n",
    "\n",
    "\"\"\"\n",
    "write to neo4j with cypher, calling the 'get_cypher_method' with the name of the node as a param\n",
    "\"\"\"\n",
    "def write_neo4j_nodes(df, node_name):\n",
    "    df.write \\\n",
    "        .format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .mode(\"Overwrite\") \\\n",
    "        .option(\"url\", config[\"bolt_url\"]) \\\n",
    "        .option(\"batch.size\", config[\"batch_size\"]) \\\n",
    "        .option(\"database\", config[\"database\"]) \\\n",
    "        .option(\"authentication.type\", \"basic\") \\\n",
    "        .option(\"authentication.basic.username\",  config[\"user\"]) \\\n",
    "        .option(\"authentication.basic.password\", config[\"password\"]) \\\n",
    "        .option(\"query\", get_cypher_query(node_name)) \\\n",
    "        .save()\n",
    "\n",
    "\"\"\"\n",
    "method to write a source and target node with a relationship using the keys stragegy - https://neo4j.com/docs/spark/current/writing/#write-rel\n",
    "The dataframe and map_model_key are passed in as parameters, the key resolves to the nodes/relationship held in the model_map_above\n",
    "\"\"\"\n",
    "def write_neo4j_node_relationship(df, model_map_key):\n",
    "    df.write \\\n",
    "        .format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"url\", config[\"bolt_url\"]) \\\n",
    "        .option(\"batch.size\", config[\"batch_size\"]) \\\n",
    "        .option(\"database\", config[\"database\"]) \\\n",
    "        .option(\"authentication.type\", \"basic\") \\\n",
    "        .option(\"authentication.basic.username\", config[\"user\"]) \\\n",
    "        .option(\"authentication.basic.password\", config[\"password\"]) \\\n",
    "        .option(\"relationship\", model_map[model_map_key][\"relationship\"][\"rel\"]) \\\n",
    "        .option(\"relationship.properties\", model_map[model_map_key][\"relationship\"][\"properties\"]) \\\n",
    "        .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "        .option(\"relationship.source.labels\", model_map[model_map_key][\"source\"][\"node\"]) \\\n",
    "        .option(\"relationship.source.save.mode\", \"overwrite\") \\\n",
    "        .option(\"relationship.source.node.keys\", model_map[model_map_key][\"source\"][\"property\"]) \\\n",
    "        .option(\"relationship.target.labels\",  model_map[model_map_key][\"target\"][\"node\"]) \\\n",
    "        .option(\"relationship.target.node.keys\", model_map[model_map_key][\"target\"][\"property\"]) \\\n",
    "        .option(\"relationship.target.save.mode\", \"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "    \n",
    "def read_neo4j_nodes(node_name):\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .option(\"url\", config[\"bolt_url\"]) \\\n",
    "        .option(\"database\", config[\"database\"]) \\\n",
    "        .option(\"authentication.type\", \"basic\") \\\n",
    "        .option(\"authentication.basic.username\", config[\"user\"]) \\\n",
    "        .option(\"authentication.basic.password\", config[\"password\"]) \\\n",
    "        .option(\"labels\", node_name) \\\n",
    "        .load() \\\n",
    "        .show()\n",
    "\n",
    "\n",
    "#use to reduce partitions if locks in neo4j occur \n",
    "#final_df = final_df.coalesce(1)\n",
    "\n",
    "#further filtering\n",
    "no_empty_strings_in_column_df = final_df.filter(\"Column1 != ''\")\n",
    "concatenate_columns_for_unique_id_df = final_df.withColumn(\"ID\", md5(regexp_replace(lower(concat(\"Column1\",\"Column2\",\"Column3\",\"Column4\", \"Column5\")), \"\\\\s+\", \"\")))\n",
    "not_null_in_column_df = final_df.filter(\"Column1 is not null\")\n",
    "\n",
    "#print out one record for debugging\n",
    "print(final_df.collect()[1])\n",
    "\n",
    "#write out nodes\n",
    "write_neo4j_nodes(final_df, \"node1\")\n",
    "write_neo4j_nodes(final_df, \"node2\")\n",
    "\n",
    "#write out nodes/relationships\n",
    "write_neo4j_node_relationship(no_empty_strings_in_column_df, \"node1_node2\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
